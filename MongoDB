# The goal of this activity is to use a K-nearest neighbors machine learning algorithm
# to train a model to predict the customer's category. Our company stores five loyalty
# categories.
# 1. Loyal customers: Customers who make up a minority of the customer base
# but who generate a large portion of sales.
# 2. Impulse customers: Customers who do not have a specific product in mind
# and purchase goods when it seems good at the time.
# 3. Discount customers: Customers who shop frequently but base buying decisions
# primarily on markdowns.
# 4. Need-based customers: Customers with the intention of buying a specific product
# based on a specific need at the time.
# 5. Wandering customers: Customers who are not sure of what they want to buy. These
# customers may be convinced to make purchases based on compelling advertisements.
#
# For this activity, the data are contained in a Mongo database located on the 172.28.8.65
# server. The name of the database is customer_loyalty and the collection is named customers.
# The fields available in the documents contained in the customers collection are the following:
# 1. custid: This field is the unique id of each customer. This field is not required for our KNN model.
# 2. category: This field is the customer category. It can be one of five categories (loyal, impulse,
# discount, need-based, or wandering.
# 3. householdincome: This field represents the total household income for all members of the customers
# household. This field might be missing for some documents.
# 4. householdsize: This field is the total number of adults living in the customer's home.
# 5. educationlevel: This field is the total number of years of education that the customer has completed.
# 6. gender: This field represents the customers gender.

import pandas as pd
import pymongo as pym
import numpy as np
import warnings
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import researchpy as rp
import os

warnings.filterwarnings('ignore')
matplotlib.use('Agg')
pd.set_option('display.max_columns', 10)

# Task 1
# Read in the data from the MongoDB and create a pandas Data Frame.

try:
    client = pym.MongoClient('172.28.8.65', 27017)
    client.server_info()
except:
    print('Something went wrong connecting to Mongo')

db = client["customer_loyalty"]
customers = db.customers
# Let's create an empty data frame
my_df = pd.DataFrame(columns=['category', 'householdincome', 'householdsize',
                              'educationlevel', 'gender'], dtype=float)
my_categories = {"loyal": 1, "impulse": 2, "discount": 3, "need-based": 4, "wandering": 5}
my_genders = {"Male": 0, "Female": 1}

# I want to use the custid field as the unique identifier (index) in my pandas Data Frame.
# To do so, I need to make sure they are unique for each document in the customers
# collection. To do this, we can use an aggregate function. What this group statement is
# doing is creating an array (list) containing all of the custids. If the custid
# appears more than one time, then the $_id will get added to the set (list) multiple times.
# If that occurs, then the "uniques" field in our output will be greater than 1 {$sum: 1}.
# We then filter ($match) down to just those results where the uniques field is greater
# than 1.
val = list(customers.aggregate([
    {"$group": {"_id": "$custid",
                "uniqueIds": {"$addToSet": "$custid"},
                "uniques": {"$sum": 1}
                }
    },
    {"$match": {"uniques": {"$gt": 1}}}]))
print(len(val))

for c in customers.find({}):
    category = my_categories[c.get('category')]
    gender = my_genders[c.get('gender')]
    my_list = [category, c.get('householdincome'),
               c.get('householdsize'), c.get('educationlevel'),
               gender]
    my_df.loc[c.get('custid')] = my_list
print(my_df)
print(my_df.dtypes)

# Task 2
# Check for nulls. We see that we have 8 nulls in the "householdincome" field.
# After consultation with the team, we decide to just remove those rows from the
# data frame.
print(my_df.isnull().sum())
my_df.dropna(inplace=True)
# my_df = my_df.dropna()
print(my_df.isnull().sum())
# Due to the nulls, the data type of householdincome may have been an object.
# As a result, let's change it to a numeric field using pd.to_numeric().
my_df['householdincome'] = pd.to_numeric(my_df.householdincome)
print(my_df.sample(5))
print(my_df.dtypes)

# Task 3
# Let's print a panel of descriptive statistics for our data. Our target is the categories
# so let's group the data by categories when printing these descriptive statistics.
print(my_df.describe())
my_fields = ['householdincome', 'householdsize', 'educationlevel', 'gender']
for field in my_fields:
    print(f'\nSummary data for {field}')
    print(rp.summary_cont(my_df[field].groupby(my_df['category'])))

# Task 4
# Let's visualize the correlations between our features and targets.
my_df.corr()
plt.close('all')
sns.pairplot(my_df, hue='category', diag_kind='hist', kind='scatter', palette='husl')
plt.savefig(os.path.join(os.getcwd(), '13_Supervised', 'KNN', 'category.png'))
plt.close('all')

# Task 5
# Create the numpy arrays necessary to feed into the knn classifier
print(my_df.columns)
my_features = my_df[['householdincome', 'householdsize', 'educationlevel', 'gender']].values
print(my_features)
# Let's suppress the scientific notation of our print out
np.set_printoptions(suppress=True)
print(my_features)
print(my_features.shape)
# For this problem, we have known targets (category field) so this a supervised machine learning problem.
my_targets = my_df[['category']].values
print(my_targets)
print(my_targets.shape)

# Task 6
# Randomly split our data into training and testing data.
# Let's make sure our analyses are reproducible and keep 25% of our data for testing.
from sklearn.model_selection import train_test_split
f_train, f_test, t_train, t_test = train_test_split(my_features, my_targets, test_size=0.25, random_state=77)

# Task 7
# The KNN algorithm is sensitive to features with different sizes
from sklearn.preprocessing import StandardScaler

# Create and fit the StandardScaler object to both the training and the testing data
sc_train = StandardScaler().fit(f_train)
f_train_sc = sc_train.transform(f_train)
sc_test = StandardScaler().fit(f_test)
f_test_sc = sc_test.transform(f_test)

# Let's convert the gender's back to the binary 0 or 1 because the
# standard scaler really only makes sense for continuous features
print(f_train_sc)
f_train_sc[:, [3]] = f_train[:, [3]]
f_test_sc[:, [3]] = f_test[:, [3]]

# If the standard scaler worked, our data should have a mean of 0 and a standard deviation of 1.
print(np.mean(f_train_sc, axis=0))
print(np.std(f_train_sc, axis=0))
print(np.mean(f_test_sc, axis=0))
print(np.std(f_test_sc, axis=0))

# Note we can back transform the data using the inverse_transform function.
# This gets us back to our original data, which will be needed if we implement this model.
print(f_train[0:4, 0:4])
for val, val_sc in zip(f_train[0:4, 0:4], f_train_sc[0:4, 0:4]):
    print(val, val_sc, sc_train.inverse_transform(val_sc.reshape(1, 4)))

# Task 8
# Let's train a KNN model with 5 neighbors, euclidean distance, and uniform weights
from sklearn import neighbors

# For this algorithm, our distance options are the following with the default being Minkowski:
# euclidean: supports the standard concept of spatial distance.
# manhattan: restricts distance measurements to follow grid lines. This metric is sometimes referred
# #          to as the Taxi cab distance, since taxis must follow streets, which
# haversine: calculates the distance travelled over the surface of a sphere, such as the Earth.
# chebyshev: assumes the distance is equal to the greatest distance along the individual dimensions.
# minkowski: a generalization of the Manhattan and Euclidean distances to arbitrary powers.

# Weights is another important hyper-parameter that we can set.
# The two most common options are:
# 1. ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.
# and
# 2. ‘distance’ : weight points by the inverse of their distance, which means that closer
# neighbors of a query point will have a greater influence than neighbors which are further away.

# Create a variable that holds the algorithm with the hyper-parameters set.
num_neighbors = 5
knn = neighbors.KNeighborsClassifier(n_neighbors=num_neighbors, metric='euclidean', weights='uniform')

# Now, train the model by passing in the training features and targets.
knn.fit(f_train_sc, t_train)

# Output the model to a pkl file.
import joblib
filename = os.path.join(os.getcwd(), '13_Supervised', 'KNN', 'knn-model_customer_types-UPDATED.pkl')
with open(filename, 'wb') as fout:
    joblib.dump(knn, fout)

# Task 9
# Let's now determine how well this model did!
# key point:
# When I am right nobody remembers but when I am wrong nobody forgets!!!
# That is how we remember and evaluate a machine learning algorithm!
#
# Compute the score & display the model accuracy with the testing data set!
score_test = 100 * knn.score(f_test_sc, t_test)
print(f'KNN ({num_neighbors} neighbors) prediction accuracy with test data = {score_test:.1f}%')

# Compute and display classification report
from sklearn.metrics import classification_report
target_names = ["loyal", "impulse", "discount", "need-based", "wandering"]

# Generate predictions
predicted_labels = knn.predict(f_test_sc)
# NOTE: Support is the sample size.
print(classification_report(t_test, predicted_labels, target_names=target_names))
# Let's display the confusion matrix
from sklearn.metrics import confusion_matrix
print(confusion_matrix(t_test, predicted_labels))

# This is a little hard to interpret because we don't have any labels indicating which axes
# are the predicted and which are the actual so let's visual the matrix.


def confusion(test, predict, title, labels, categories):
    """Plot the confusion matrix to make it easier to interpret.
       This function produces a colored heatmap that displays the relationship
        between predicted and actual types from a machine learning method."""

    # Make a 2D histogram from the test and result arrays.
    # pts is essentially the output of the scikit-learn confusion_matrix method.
    pts, xe, ye = np.histogram2d(test, predict, bins=categories)

    # For simplicity we create a new DataFrame for the confusion matrix.
    pd_pts = pd.DataFrame(pts.astype(int), index=labels, columns=labels)

    # Display heatmap and add decorations.
    hm = sns.heatmap(pd_pts, annot=True, fmt="d")
    hm.axes.set_title(title, fontsize=20)
    hm.axes.set_xlabel('True Target', fontsize=18)
    hm.axes.set_ylabel('Predicted Label', fontsize=18)
    # I might sometimes return None if I want to have an if conditional associated with
    # the output of this function. For this example, it is not really needed!
    # return None


# Call confusion matrix plotting routine
plt.close('all')
confusion(t_test.flatten(), predicted_labels, f'KNN-({num_neighbors}) Model', target_names, 5)
plt.savefig(os.path.join(os.getcwd(), '13_Supervised', 'KNN', 'confusion_customer_types.png'))
plt.close('all')

# Task 10
# Determine if we have an over-fitting problem with this model. That is, did the model perform better in training
# than it did in testing.
#
score_test = 100 * knn.score(f_test_sc, t_test)
print(f'KNN ({num_neighbors} neighbors) prediction accuracy with test data = {score_test:.1f}%')

score_train = 100 * knn.score(f_train_sc, t_train)
print(f'KNN ({num_neighbors} neighbors) prediction accuracy with training data = {score_train:.1f}%')

# Task 11
# Display the predicted customer category and the associated probabilities
# for the third row (2nd indexed row) in the test data
target_names = ["loyal", "impulse", "discount", "need-based", "wandering"]
my_feature_names = ['household income', 'household size', 'education level', 'gender']
my_gender_names = ["Male", "Female"]
test_row = 2
predictions = knn.predict(f_test_sc[test_row, :].reshape(1, 4))

print(f'This knn model predicts that a customer with {f_test_sc[test_row, :]} values'
      f' will be a {predictions[0]} category, which is '
      f'a {target_names[int(predictions[0])]} category customer.')
# This output is only moderately helpful because the f_test_sc are the
# standardized values. It would be more meaningful to see the actual (non-transformed values)
# in this output. Therefore, let's back transform to the actual values to be more helpful.
# We can use the sc_test Scaler that we previously created to accomplish this task.
back_transformed = sc_test.inverse_transform(f_test_sc[test_row, :].reshape(1, 4))
print(back_transformed)
# Fix the gender because those are not on the standardized scaler, which is common for categorical data.
back_transformed[0][3] = f_test[test_row, 3]
print(back_transformed, f_test_sc[test_row, :])

output = 'This knn model predicts that a customer with '
for index, elem in np.ndenumerate(back_transformed):
    if index[1] == 3:
        output += f'and is a {my_gender_names[int(elem)]} '
    else:
        output += f'{my_feature_names[index[1]]} of {elem}, '
output += f'will be a {predictions[0]} category, which is ' \
          f'a "{target_names[int(predictions[0])]}" category customer.'
print(output)

# Now, let's determine how confident that this model is with its prediction.
probabilities = knn.predict_proba(f_test_sc[test_row].reshape(1, 4))
print(probabilities)

output = 'This model estimates that there is a \n'
for index, elem in np.ndenumerate(probabilities):
    output += f'{elem * 100}% chance that this customer will ' \
              f'be in the {target_names[index[1]]} category\n'
print(output)

# Task 12
# Which set of hyper-parameters for distance, weights, and k fit our data the best?
# We can use the GridSearchCV to accomplish this objective!
from sklearn.model_selection import GridSearchCV
hyper_parameters = {'n_neighbors': (3, 5, 7, 9, 11),
                    'metric': ['euclidean', 'manhattan', 'haversine', 'chebyshev', 'minkowski'],
                    'weights': ['distance', 'uniform']}
knn = neighbors.KNeighborsClassifier()
clf = GridSearchCV(knn, hyper_parameters)
clf.fit(f_train_sc, t_train)
print(sorted(clf.cv_results_.keys()))
scores = clf.cv_results_['mean_test_score']
print(scores)
best_k = clf.best_params_['n_neighbors']
print(best_k)
best_metric = clf.best_params_['metric']
print(best_metric)
best_weight = clf.best_params_['weights']
print(best_weight)

score_test = 100 * clf.score(f_test_sc, t_test)
print(f'Grid search prediction accuracy with test data = {score_test:.1f}%')

predicted_labels = clf.predict(f_test_sc)
print(classification_report(t_test, predicted_labels, target_names=target_names))

# Create and display confusion matrix
print(confusion_matrix(t_test, predicted_labels))
plt.close('all')
confusion(t_test.flatten(), predicted_labels,
          f'KNN-(Best k:{best_k}, Best distance: {best_metric}, Best Weight: {best_weight}) Model',
          target_names, 5)
plt.savefig(os.path.join(os.getcwd(), '13_Supervised', 'KNN', 'confusion_customer_types_gridsearch.png'))
plt.close('all')

# Not in the question, but let's save the model.
import joblib
filename = os.path.join(os.getcwd(), '13_Supervised', 'KNN', 'knn-model_customer_types.pkl')
with open(filename, 'wb') as fout:
    joblib.dump(knn, fout)

# Open model file (.pkl) and load model.
with open(filename, 'rb') as fin:
    knn_from_pkl = joblib.load(fin)
